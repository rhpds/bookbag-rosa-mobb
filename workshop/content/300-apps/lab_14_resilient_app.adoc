= Making an application resilient

In this section of the workshop, we will deploy an application to an ROSA cluster, ensure the application is resilient to node failure, and scale the application when under load.

== Deploy an application

. First, let's deploy an application.
To do so, run the following set of commands:
+
[source,sh,role=execute]
----
oc new-project resilience-ex

oc -n resilience-ex new-app https://github.com/rh-mobb/frontend-js.git --name frontend-js

oc -n resilience-ex expose svc frontend-js

oc -n resilience-ex set resources deployment/frontend-js \
  --limits=cpu=20m,memory=150Mi \
  --requests=cpu=20m,memory=100Mi
----

. While the application is being built from source, you can watch the rollout status of the deployment object to see when its finished.
+
[source,sh,role=execute]
----
oc rollout status deploy/frontend-js -n resilience-ex
----
+
.Sample Output
[source,text,options=nowrap]
----
Waiting for deployment "frontend-js" rollout to finish: 0 out of 1 new replicas have been updated...
Waiting for deployment spec update to be observed...
Waiting for deployment spec update to be observed...
Waiting for deployment "frontend-js" rollout to finish: 0 out of 1 new replicas have been updated...
Waiting for deployment "frontend-js" rollout to finish: 0 out of 1 new replicas have been updated...
Waiting for deployment "frontend-js" rollout to finish: 0 of 1 updated replicas are available...
deployment "frontend-js" successfully rolled out
----

. We can now use the route to view the application in your web browser.
To get the route, run the following command:
+
[source,sh,role=execute]
----
oc -n resilience-ex get route frontend-js \
  -o jsonpath='http://{.spec.host}{"\n"}'
----
+
.Sample Output
[source,text,options=nowrap]
----
http://frontend-js-resilience-ex.apps.rosa-6n4s8.1c1c.p1.openshiftapps.com
----
+
Then visit the URL presented in a new tab in your web browser (using HTTP).

. Initially, this application is deployed with only one pod.
In the event a worker node goes down or the pod crashes, there will be an outage of the application.
To prevent that, let's scale the number of instances of our applications up to three.
To do so, run the following command:
+
[source,sh,role=execute]
----
oc -n resilience-ex scale deployment \
  frontend-js --replicas=3
----
+
.Sample Output
[source,text,options=nowrap]
----
deployment.apps/frontend-js scaled
----

. Next, check to see that the application has scaled.
To do so, run the following command to see the pods.
Then check that it has scaled
+
[source,sh,role=execute]
----
oc -n resilience-ex get pods \
  -l deployment=frontend-js
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME                           READY   STATUS    RESTARTS   AGE
frontend-js-64c74c565f-hr562   1/1     Running   0          87s
frontend-js-64c74c565f-kw55k   1/1     Running   0          22s
frontend-js-64c74c565f-pbfmz   1/1     Running   0          22s
----

== Pod Disruption Budget

A Pod disruption Budget (PBD) allows you to limit the disruption to your application when its pods need to be rescheduled for upgrades or routine maintenance work on ROSA nodes.
In essence, it lets developers define the minimum tolerable operational requirements for a deployment so that it remains stable even during a disruption.

For example, frontend-js deployed as part of the last step contains three replicas distributed evenly across three nodes.
We can tolerate losing two pods but not one, so we create a PDB that requires a minimum of one replica.

A PodDisruptionBudget object's configuration consists of the following key parts:

* A label selector, which is a label query over a set of pods.
* An availability level, which specifies the minimum number of pods that must be available simultaneously, either:
** minAvailable is the number of pods must always be available, even during a disruption.
** maxUnavailable is the number of pods can be unavailable during a disruption.

[WARNING]
====
A maxUnavailable of 0% or 0 or a minAvailable of 100% or equal to the number of replicas can be used but will block nodes from being drained and can result in application instability during maintenance activities.
====

. Let's create a Pod Disruption Budget for our `frontend-js` application.
To do so, run the following command:
+
[source,sh,role=execute]
----
cat <<EOF | oc apply -f -
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: frontend-js-pdb
  namespace: resilience-ex
spec:
  minAvailable: 1
  selector:
    matchLabels:
      deployment: frontend-js
EOF
----
+
.Sample Output
[source,text,options=nowrap]
----
poddisruptionbudget.policy/frontend-js-pdb created
----
+
After creating the PDB, OpenShift API will ensure at least one pod of `frontend-js` is running all the time, even when maintenance is going on with the cluster.

. Next, let's check the status of Pod Disruption Budget.
To do so, run the following command:
+
[source,sh,role=execute]
----
oc -n resilience-ex get poddisruptionbudgets
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME              MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
frontend-js-pdb   1               N/A               2                     20s
----

== Horizontal Pod Autoscaler (HPA)

As a developer, you can use a horizontal pod autoscaler (HPA) to specify how ROSA clusters should automatically increase or decrease the scale of a replication controller or deployment configuration, based on metrics collected from the pods that belong to that replication controller or deployment configuration.
You can create an HPA for any deployment, replica set, replication controller, or stateful set.

In this exercise we will scale the `frontend-js` application based on CPU utilization:

* Scale out when average CPU utilization is greater than 50% of CPU limit
* Maximum pods is 4
* Scale down to min replicas if utilization is lower than threshold for 60 sec

. First, we should create the HorizontalPodAutoscaler.
To do so, run the following command:
+
[source,sh,role=execute]
----
cat <<EOF | oc apply -f -
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: frontend-js-cpu
  namespace: resilience-ex
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: frontend-js
  minReplicas: 2
  maxReplicas: 4
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        averageUtilization: 50
        type: Utilization
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
EOF
----
+
.Sample Output
[source,text,options=nowrap]
----
horizontalpodautoscaler.autoscaling/frontend-js-cpu created
----

. Next, check the status of the HPA.
To do so, run the following command:
+
[source,sh,role=execute]
----
oc -n resilience-ex get horizontalpodautoscaler/frontend-js-cpu
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME              REFERENCE                TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
frontend-js-cpu   Deployment/frontend-js   0%/50%    2         4         2          4m28s
----
+
Note that it may take a while to update the *TARGETS* column with actual values. You can continue the lab regardless without waiting for the TARGETS column to show any metrics.

. Next, let's generate some load against the `frontend-js` application.
+
First you need to install the `siege` tool to your bastion VM:
+
[source,sh,role=execute]
----
mkdir -p $HOME/bin
curl -o $HOME/bin/siege https://gpte-public.s3.amazonaws.com/siege
chmod +x $HOME/bin/siege
mkdir $HOME/.siege
----

. Create a configuration file for the `siege` tool:
+
[source,sh,role=execute]
----
cat << EOF >$HOME/.siege/siege.conf
verbose = true
color = on
quiet = false
json_output = false
EOF
----

. Now you can generate some load on your application to trigger the autoscaler:
+
[source,sh,role=execute]
----
FRONTEND_URL=http://$(oc -n resilience-ex get route frontend-js -o jsonpath='{.spec.host}')

siege -c 255 $FRONTEND_URL
----

. Wait for a minute and then kill the siege command (by hitting CTRL and c on your keyboard).
Then immediately check the status of Horizontal Pod Autoscaler.
To do so, run the following command:
+
[source,sh,role=execute]
----
oc -n resilience-ex get horizontalpodautoscaler/frontend-js-cpu
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME              REFERENCE                TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
frontend-js-cpu   Deployment/frontend-js   24%/50%   2         4         3          62m
----
+
This means you are now running 3 replicas, instead of the original two that we started with.

. Once you've killed the siege command, the traffic going to `frontend-js` service will cool down and after a 60 second cool down period, your application's replica count will drop back down to two.
To demonstrate this, run the following command:
+
[source,sh,role=execute]
----
oc -n resilience-ex get horizontalpodautoscaler/frontend-js-cpu --watch
----
+
.After a minute or two, your output should be similar to this
[source,text,options=nowrap]
----
NAME              REFERENCE                TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
frontend-js-cpu   Deployment/frontend-js   10%/50%   2         4         4          6m55s
frontend-js-cpu   Deployment/frontend-js   8%/50%    2         4         4          7m1s
frontend-js-cpu   Deployment/frontend-js   8%/50%    2         4         3          7m16s
frontend-js-cpu   Deployment/frontend-js   0%/50%    2         4         2          7m31s
----
